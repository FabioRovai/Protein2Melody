{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Copy of Copy of audiodna.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIBl5ggFxmtd"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxD8Fumb5SJc",
        "outputId": "4496edc0-fa74-40b7-a4b5-95b393998bd5"
      },
      "source": [
        "!pip install Pillow numpy opencv-python PyWavelets tqdm slugify -q\n",
        "!pip install -U Flask -q\n",
        "!pip install lws==1.2.6 -q\n",
        "!pip install tflearn -q\n",
        "!pip install librosa==0.7.2 -q\n",
        "!pip install numba==0.48 -q\n",
        "!pip install mock -q \n",
        "!pip install pydub -q\n",
        "from pydub import AudioSegment\n",
        "import tensorflow as tf\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "import soundfile as sf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from scipy.constants import speed_of_light as speedlight\n",
        "from unittest.mock import Mock, MagicMock\n",
        "\n",
        "# Collect amino acid vibrational frequencies as wavenumbers\n",
        "!wget https://raw.githubusercontent.com/frodoCombs/Amino_Acid_Frequencies/master/aa_freq.csv -q\n",
        "#Ual\n",
        "!git clone https://github.com/ual-cci/music_gen_interaction_RTML.git -q\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for slugify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 153kB 17.9MB/s \n",
            "\u001b[?25h  Building wheel for lws (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 19.5MB/s \n",
            "\u001b[?25h  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 3.6MB 19.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 20.2MB 1.3MB/s \n",
            "\u001b[31mERROR: umap-learn 0.5.1 has requirement numba>=0.49, but you'll have numba 0.48.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pynndescent 0.5.2 has requirement numba>=0.51.2, but you'll have numba 0.48.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-3yootfdXg0"
      },
      "source": [
        "# Dictionary to be used for converting amino acid abbreviations\n",
        "aa_3to1 = {'CYS': 'C', 'ASP': 'D', 'SER': 'S',\n",
        "'GLN': 'Q', 'LYS':'K','ILE': 'I', 'PRO': 'P',\n",
        " 'THR': 'T', 'PHE': 'F', 'ASN': 'N', 'GLY': 'G',\n",
        " 'HIS': 'H', 'LEU': 'L', 'ARG': 'R', 'TRP': 'W',\n",
        " 'ALA': 'A', 'VAL':'V', 'GLU': 'E', 'TYR': 'Y', 'MET': 'M'}\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/aa_freq.csv', sep=',',header=0)\n",
        "\n",
        "\n",
        "df.fillna(0,inplace=True)\n",
        "\n",
        "\n",
        "#convert to np array\n",
        "df = df.values\n",
        "for ro in range(df.shape[0]):\n",
        "    for co in range(df.shape[1]):\n",
        "        if co == 0:\n",
        "        #Convert amino acid code from 3 letter to 1 letter\n",
        "            aa = df[ro][co]\n",
        "            aa = aa[0:3].upper()\n",
        "            df[ro][co]= aa_3to1[aa]\n",
        "        else:\n",
        "        # Convert from wavenumber to frequency\n",
        "        # wavelength = 1/wavenumber\n",
        "        # frequency = speed of light / wavelength\n",
        "            if df[ro][co] > 0:\n",
        "                freq = speedlight / (1/df[ro][co])\n",
        "\n",
        "\n",
        "aa_list = df[:,0]      # list of amino acids\n",
        "df = df[:,1:]          # remove first column                \n",
        "max_freq = df.max()    # max and min \n",
        "min_freq = df.min()    # need to exclude first column\n",
        "human_range = 20000 - 20\n",
        "for ro in range(df.shape[0]):\n",
        "    for co in range(df.shape[1]):\n",
        "        if df[ro][co] > 0:\n",
        "            # First scale to 0 to 1\n",
        "            df[ro][co] = (df[ro][co] - min_freq)/ max_freq - min_freq\n",
        "            # Scale to human hearing range 20 - 20000 Hz\n",
        "            df[ro][co] = (df[ro][co] * human_range) + 20\n",
        "\n",
        "def aa_note(aa,length):\n",
        "    row = np.where(aa_list == aa)[0][0]\n",
        "    wave = np.zeros(int(44100 * float(length)))\n",
        "    harm = 0\n",
        " \n",
        "    for freq in df[row]:\n",
        "        if freq == 0:\n",
        "            break\n",
        "        else:\n",
        "            volume = np.exp(-harm)\n",
        "            phases = np.cumsum(2.0 * np.pi * freq / 44100 * np.ones(int(44100 * float(length))))\n",
        "            wave += np.sin(phases) * volume\n",
        "            harm += 1\n",
        "    # scale wave to values between 1 and -1\n",
        "    wave = 2.*(wave - wave.min())/np.ptp(wave)-1\n",
        "    return wave\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un12v6tZc7Nz"
      },
      "source": [
        "# a sample protein sequence\n",
        "#https://www.bioinformatics.org/sms2/random_protein.html\n",
        "protein_sequence = 'VKMSHPSGDVEAILACYEGCCPGSVYGNVVAFLYAALINGERWFVTNTSMSLESYNTVKMSHPSGDVEAILACYEGCCPGSVYGNVVAFLYAALINGERWFVTNTSMSLESYNVKMSHPSGDVEAILACYEGCCPGSVYGNVVAFLYAALINGERWFVTNTSMSLESYN'\n",
        "\n",
        "unit_length = 0.5\n",
        "melodic_sequence = []\n",
        "for residue in protein_sequence:\n",
        "    sound_wave = aa_note(residue,unit_length)\n",
        "    melodic_sequence = melodic_sequence + sound_wave.tolist()\n",
        "melody = np.asarray(melodic_sequence)\n",
        "melody = 2*(melody - melody.min())/np.ptp(melody)-1\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EiL1rTj0Dd0"
      },
      "source": [
        "sr=22050\n",
        "def show_audio(y, sr=220500, xlim=5000):\n",
        "    fig, ax = plt.subplots(figsize=(12,4))\n",
        "    ax.set_xlim(0,xlim)\n",
        "    ax.set_ylim(-1,1)\n",
        "    ax.plot(y)\n",
        "    return ipd.Audio(y, rate=sr,  autoplay=True)\n",
        "    \n",
        "sf.write('long_sample.wav', melody, sr)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYBEiwAkKwEJ",
        "outputId": "da360c96-0c10-42fa-e9c4-a32eed2918c5"
      },
      "source": [
        "librosa.get_duration(melody, sr)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "169.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mbldhZJ9WpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae5dd504-70ba-468a-c7da-7ba26fecb664"
      },
      "source": [
        "%cd /content/music_gen_interaction_RTML/\n",
        "\n",
        "args = MagicMock(name='method')\n",
        "sample_rate = 22050\n",
        "\n",
        "# This will set the same setting for the training and the running of the model:\n",
        "args.lstm_layers = 3\n",
        "args.lstm_units = 128\n",
        "args.sample_rate = sample_rate\n",
        "args.sequence_length = 40\n",
        "args.async_loading = True\n",
        "args.amount_epochs = -1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/music_gen_interaction_RTML\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ljxd0cDoDuxD",
        "outputId": "0ac5f033-147e-4f37-e5db-b308ca76278e"
      },
      "source": [
        "# Time to miliseconds\n",
        "startTime = 1*1000\n",
        "endTime = 61*1000\n",
        "\n",
        "# Opening file and extracting segment\n",
        "song = AudioSegment.from_wav( '/content/long_sample.wav' )\n",
        "extract = song[startTime:endTime]\n",
        "\n",
        "# Saving\n",
        "extract.export( '/content/music_gen_interaction_RTML/sample.wav', format=\"wav\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.BufferedRandom name='/content/music_gen_interaction_RTML/sample.wav'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCo3haVB_f4G"
      },
      "source": [
        "number_of_epochs = 300 # will take cca 8min\n",
        "\n",
        "song_name = \"sample.wav\" # < we will train the model on this wav file\n",
        "model_name = \"my_trained_model\" # < and then save the model under this name (check the .tfl files in the folder afterwards)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXbgYJXh-Js1",
        "outputId": "37c1e9d5-c646-4b5a-cef9-9956199f875f"
      },
      "source": [
        "# ----[keep the same bellow]-------------------------------------------------------------------\n",
        "\n",
        "# takes time!\n",
        "!python training_handler.py -target_file $song_name -model_name $model_name -amount_epochs $number_of_epochs -batch_size 512 \\\n",
        "                            -lstm_layers $args.lstm_layers  -lstm_units $args.lstm_units -sample_rate $args.sample_rate -sequence_length $args.sequence_length\n",
        "from IPython.display import clear_output \n",
        "clear_output()\n",
        "\n",
        "import glob\n",
        "import numpy as np\n",
        "i = glob.glob(\"*.tfl.png\")\n",
        "from IPython.display import Audio, Image\n",
        "display(Image(i[0]))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Settings:\n",
            "\t- server_model_paths_start: __saved_models/\n",
            "\t- server_songs_paths_start: __music_samples/\n",
            "Server settings: settings.lstm_layers= 3 , settings.lstm_units= 128 , settings.griffin_iterations= 60 , settings.sample_rate= 22050\n",
            "[[[[ Training on music file sample.wav  - will save the model as  my_trained_model\n",
            "Zero-padding symmetrically around the original windows.\n",
            "WARNING: for code simplicity, a consequence is that the first/last 512 samples of the signal will not be in the perfect reconstruction region.\n",
            "100% data generation complete.Dataset: (2623, 40, 1025) (2623, 1025)\n",
            "Loaded dataset.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/layers/core.py:81: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/layers/normalization.py:59: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/layers/recurrent.py:69: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/layers/recurrent.py:553: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/layers/core.py:236: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/initializations.py:174: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/optimizers.py:238: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/layers/estimator.py:189: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/helpers/trainer.py:571: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/helpers/trainer.py:115: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2021-02-28 11:45:17.247374: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-02-28 11:45:17.302640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-28 11:45:17.303370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-02-28 11:45:17.303755: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-02-28 11:45:17.517976: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-02-28 11:45:17.666326: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-02-28 11:45:17.680980: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-02-28 11:45:17.958631: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-02-28 11:45:17.972221: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-02-28 11:45:18.476068: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-02-28 11:45:18.476321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-28 11:45:18.477043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-28 11:45:18.477572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-02-28 11:45:18.487063: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-02-28 11:45:18.487367: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556d49e8f6c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-02-28 11:45:18.487402: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2021-02-28 11:45:18.666486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-28 11:45:18.667200: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556d49e8f880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2021-02-28 11:45:18.667234: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2021-02-28 11:45:18.667458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-28 11:45:18.667982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-02-28 11:45:18.668076: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-02-28 11:45:18.668106: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-02-28 11:45:18.668136: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-02-28 11:45:18.668159: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-02-28 11:45:18.668182: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-02-28 11:45:18.668207: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-02-28 11:45:18.668233: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-02-28 11:45:18.668329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-28 11:45:18.668900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-28 11:45:18.669412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-02-28 11:45:18.672593: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-02-28 11:45:18.674981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-02-28 11:45:18.675015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-02-28 11:45:18.675027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-02-28 11:45:18.675924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-28 11:45:18.676592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-28 11:45:18.677128: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-02-28 11:45:18.677169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/summaries.py:46: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/summaries.py:44: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/helpers/trainer.py:134: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/helpers/trainer.py:164: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/helpers/trainer.py:165: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/helpers/trainer.py:166: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tflearn/helpers/trainer.py:167: The name tf.get_collection_ref is deprecated. Please use tf.compat.v1.get_collection_ref instead.\n",
            "\n",
            "2021-02-28 11:45:31.565737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-28 11:45:31.566341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-02-28 11:45:31.566440: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-02-28 11:45:31.566469: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-02-28 11:45:31.566495: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-02-28 11:45:31.566518: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-02-28 11:45:31.566546: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-02-28 11:45:31.566569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-02-28 11:45:31.566591: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-02-28 11:45:31.566698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-28 11:45:31.567315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-28 11:45:31.567830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-02-28 11:45:31.567872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-02-28 11:45:31.567888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-02-28 11:45:31.567897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-02-28 11:45:31.568000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-28 11:45:31.568558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-28 11:45:31.569130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "Created model.\n",
            "Model = | my_trained_model.tfl |\n",
            "---------------------------------\n",
            "Run id: V5K4ZL\n",
            "Log directory: /tmp/tflearn_logs/\n",
            "WARNING:tensorflow:Issue encountered when serializing layer_tensor/LSTM.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'list' object has no attribute 'name'\n",
            "WARNING:tensorflow:Issue encountered when serializing layer_tensor/Dropout.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'list' object has no attribute 'name'\n",
            "WARNING:tensorflow:Issue encountered when serializing layer_tensor/LSTM_1.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'list' object has no attribute 'name'\n",
            "---------------------------------\n",
            "Training samples: 2623\n",
            "Validation samples: 0\n",
            "--\n",
            "2021-02-28 11:45:38.151582: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "Training Step: 1  | time: 7.617s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 0512/2623\n",
            "Training Step: 2  | total loss: \u001b[1m\u001b[32m81.00599\u001b[0m\u001b[0m | time: 7.900s\n",
            "| Adam | epoch: 001 | loss: 81.00599 - acc: 0.0000 -- iter: 1024/2623\n",
            "Training Step: 3  | total loss: \u001b[1m\u001b[32m89.76938\u001b[0m\u001b[0m | time: 8.178s\n",
            "| Adam | epoch: 001 | loss: 89.76938 - acc: 0.2988 -- iter: 1536/2623\n",
            "Training Step: 4  | total loss: \u001b[1m\u001b[32m90.20222\u001b[0m\u001b[0m | time: 8.445s\n",
            "| Adam | epoch: 001 | loss: 90.20222 - acc: 0.2622 -- iter: 2048/2623\n",
            "Training Step: 5  | total loss: \u001b[1m\u001b[32m89.93346\u001b[0m\u001b[0m | time: 8.743s\n",
            "| Adam | epoch: 001 | loss: 89.93346 - acc: 0.1848 -- iter: 2560/2623\n",
            "Training Step: 6  | total loss: \u001b[1m\u001b[32m89.41153\u001b[0m\u001b[0m | time: 8.937s\n",
            "| Adam | epoch: 001 | loss: 89.41153 - acc: 0.1376 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 7  | total loss: \u001b[1m\u001b[32m88.46935\u001b[0m\u001b[0m | time: 0.194s\n",
            "| Adam | epoch: 002 | loss: 88.46935 - acc: 0.0646 -- iter: 0512/2623\n",
            "Training Step: 8  | total loss: \u001b[1m\u001b[32m87.42825\u001b[0m\u001b[0m | time: 0.452s\n",
            "| Adam | epoch: 002 | loss: 87.42825 - acc: 0.0282 -- iter: 1024/2623\n",
            "Training Step: 9  | total loss: \u001b[1m\u001b[32m86.60145\u001b[0m\u001b[0m | time: 0.723s\n",
            "| Adam | epoch: 002 | loss: 86.60145 - acc: 0.0133 -- iter: 1536/2623\n",
            "Training Step: 10  | total loss: \u001b[1m\u001b[32m85.46311\u001b[0m\u001b[0m | time: 0.986s\n",
            "| Adam | epoch: 002 | loss: 85.46311 - acc: 0.0066 -- iter: 2048/2623\n",
            "Training Step: 11  | total loss: \u001b[1m\u001b[32m84.18180\u001b[0m\u001b[0m | time: 1.249s\n",
            "| Adam | epoch: 002 | loss: 84.18180 - acc: 0.0035 -- iter: 2560/2623\n",
            "Training Step: 12  | total loss: \u001b[1m\u001b[32m82.15748\u001b[0m\u001b[0m | time: 1.516s\n",
            "| Adam | epoch: 002 | loss: 82.15748 - acc: 0.0019 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 13  | total loss: \u001b[1m\u001b[32m80.19658\u001b[0m\u001b[0m | time: 0.199s\n",
            "| Adam | epoch: 003 | loss: 80.19658 - acc: 0.0019 -- iter: 0512/2623\n",
            "Training Step: 14  | total loss: \u001b[1m\u001b[32m78.18316\u001b[0m\u001b[0m | time: 0.382s\n",
            "| Adam | epoch: 003 | loss: 78.18316 - acc: 0.0011 -- iter: 1024/2623\n",
            "Training Step: 15  | total loss: \u001b[1m\u001b[32m76.32951\u001b[0m\u001b[0m | time: 0.650s\n",
            "| Adam | epoch: 003 | loss: 76.32951 - acc: 0.0007 -- iter: 1536/2623\n",
            "Training Step: 16  | total loss: \u001b[1m\u001b[32m75.02784\u001b[0m\u001b[0m | time: 0.894s\n",
            "| Adam | epoch: 003 | loss: 75.02784 - acc: 0.0026 -- iter: 2048/2623\n",
            "Training Step: 17  | total loss: \u001b[1m\u001b[32m73.75461\u001b[0m\u001b[0m | time: 1.177s\n",
            "| Adam | epoch: 003 | loss: 73.75461 - acc: 0.0017 -- iter: 2560/2623\n",
            "Training Step: 18  | total loss: \u001b[1m\u001b[32m72.31754\u001b[0m\u001b[0m | time: 1.449s\n",
            "| Adam | epoch: 003 | loss: 72.31754 - acc: 0.0018 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 19  | total loss: \u001b[1m\u001b[32m70.84319\u001b[0m\u001b[0m | time: 0.258s\n",
            "| Adam | epoch: 004 | loss: 70.84319 - acc: 0.0012 -- iter: 0512/2623\n",
            "Training Step: 20  | total loss: \u001b[1m\u001b[32m69.51250\u001b[0m\u001b[0m | time: 0.439s\n",
            "| Adam | epoch: 004 | loss: 69.51250 - acc: 0.0014 -- iter: 1024/2623\n",
            "Training Step: 21  | total loss: \u001b[1m\u001b[32m67.63409\u001b[0m\u001b[0m | time: 0.646s\n",
            "| Adam | epoch: 004 | loss: 67.63409 - acc: 0.0010 -- iter: 1536/2623\n",
            "Training Step: 22  | total loss: \u001b[1m\u001b[32m66.04475\u001b[0m\u001b[0m | time: 0.892s\n",
            "| Adam | epoch: 004 | loss: 66.04475 - acc: 0.0007 -- iter: 2048/2623\n",
            "Training Step: 23  | total loss: \u001b[1m\u001b[32m65.10136\u001b[0m\u001b[0m | time: 1.123s\n",
            "| Adam | epoch: 004 | loss: 65.10136 - acc: 0.0016 -- iter: 2560/2623\n",
            "Training Step: 24  | total loss: \u001b[1m\u001b[32m64.21522\u001b[0m\u001b[0m | time: 1.377s\n",
            "| Adam | epoch: 004 | loss: 64.21522 - acc: 0.0017 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 25  | total loss: \u001b[1m\u001b[32m63.43241\u001b[0m\u001b[0m | time: 0.251s\n",
            "| Adam | epoch: 005 | loss: 63.43241 - acc: 0.0023 -- iter: 0512/2623\n",
            "Training Step: 26  | total loss: \u001b[1m\u001b[32m62.74671\u001b[0m\u001b[0m | time: 0.512s\n",
            "| Adam | epoch: 005 | loss: 62.74671 - acc: 0.0022 -- iter: 1024/2623\n",
            "Training Step: 27  | total loss: \u001b[1m\u001b[32m61.88704\u001b[0m\u001b[0m | time: 0.696s\n",
            "| Adam | epoch: 005 | loss: 61.88704 - acc: 0.0016 -- iter: 1536/2623\n",
            "Training Step: 28  | total loss: \u001b[1m\u001b[32m61.30832\u001b[0m\u001b[0m | time: 0.882s\n",
            "| Adam | epoch: 005 | loss: 61.30832 - acc: 0.0012 -- iter: 2048/2623\n",
            "Training Step: 29  | total loss: \u001b[1m\u001b[32m60.68810\u001b[0m\u001b[0m | time: 1.131s\n",
            "| Adam | epoch: 005 | loss: 60.68810 - acc: 0.0009 -- iter: 2560/2623\n",
            "Training Step: 30  | total loss: \u001b[1m\u001b[32m59.81244\u001b[0m\u001b[0m | time: 1.374s\n",
            "| Adam | epoch: 005 | loss: 59.81244 - acc: 0.0007 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 31  | total loss: \u001b[1m\u001b[32m58.96259\u001b[0m\u001b[0m | time: 0.255s\n",
            "| Adam | epoch: 006 | loss: 58.96259 - acc: 0.0010 -- iter: 0512/2623\n",
            "Training Step: 32  | total loss: \u001b[1m\u001b[32m58.20905\u001b[0m\u001b[0m | time: 0.502s\n",
            "| Adam | epoch: 006 | loss: 58.20905 - acc: 0.0012 -- iter: 1024/2623\n",
            "Training Step: 33  | total loss: \u001b[1m\u001b[32m57.72452\u001b[0m\u001b[0m | time: 0.765s\n",
            "| Adam | epoch: 006 | loss: 57.72452 - acc: 0.0014 -- iter: 1536/2623\n",
            "Training Step: 34  | total loss: \u001b[1m\u001b[32m57.04189\u001b[0m\u001b[0m | time: 0.949s\n",
            "| Adam | epoch: 006 | loss: 57.04189 - acc: 0.0015 -- iter: 2048/2623\n",
            "Training Step: 35  | total loss: \u001b[1m\u001b[32m56.59273\u001b[0m\u001b[0m | time: 1.148s\n",
            "| Adam | epoch: 006 | loss: 56.59273 - acc: 0.0012 -- iter: 2560/2623\n",
            "Training Step: 36  | total loss: \u001b[1m\u001b[32m56.14547\u001b[0m\u001b[0m | time: 1.394s\n",
            "| Adam | epoch: 006 | loss: 56.14547 - acc: 0.0009 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 37  | total loss: \u001b[1m\u001b[32m55.58553\u001b[0m\u001b[0m | time: 0.232s\n",
            "| Adam | epoch: 007 | loss: 55.58553 - acc: 0.0008 -- iter: 0512/2623\n",
            "Training Step: 38  | total loss: \u001b[1m\u001b[32m54.92211\u001b[0m\u001b[0m | time: 0.483s\n",
            "| Adam | epoch: 007 | loss: 54.92211 - acc: 0.0014 -- iter: 1024/2623\n",
            "Training Step: 39  | total loss: \u001b[1m\u001b[32m54.53154\u001b[0m\u001b[0m | time: 0.750s\n",
            "| Adam | epoch: 007 | loss: 54.53154 - acc: 0.0015 -- iter: 1536/2623\n",
            "Training Step: 40  | total loss: \u001b[1m\u001b[32m54.02808\u001b[0m\u001b[0m | time: 1.006s\n",
            "| Adam | epoch: 007 | loss: 54.02808 - acc: 0.0012 -- iter: 2048/2623\n",
            "Training Step: 41  | total loss: \u001b[1m\u001b[32m53.67530\u001b[0m\u001b[0m | time: 1.190s\n",
            "| Adam | epoch: 007 | loss: 53.67530 - acc: 0.0013 -- iter: 2560/2623\n",
            "Training Step: 42  | total loss: \u001b[1m\u001b[32m53.48227\u001b[0m\u001b[0m | time: 1.372s\n",
            "| Adam | epoch: 007 | loss: 53.48227 - acc: 0.0011 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 43  | total loss: \u001b[1m\u001b[32m53.27515\u001b[0m\u001b[0m | time: 0.234s\n",
            "| Adam | epoch: 008 | loss: 53.27515 - acc: 0.0009 -- iter: 0512/2623\n",
            "Training Step: 44  | total loss: \u001b[1m\u001b[32m52.92078\u001b[0m\u001b[0m | time: 0.478s\n",
            "| Adam | epoch: 008 | loss: 52.92078 - acc: 0.0007 -- iter: 1024/2623\n",
            "Training Step: 45  | total loss: \u001b[1m\u001b[32m52.52103\u001b[0m\u001b[0m | time: 0.751s\n",
            "| Adam | epoch: 008 | loss: 52.52103 - acc: 0.0006 -- iter: 1536/2623\n",
            "Training Step: 46  | total loss: \u001b[1m\u001b[32m52.12746\u001b[0m\u001b[0m | time: 1.026s\n",
            "| Adam | epoch: 008 | loss: 52.12746 - acc: 0.0008 -- iter: 2048/2623\n",
            "Training Step: 47  | total loss: \u001b[1m\u001b[32m52.00125\u001b[0m\u001b[0m | time: 1.301s\n",
            "| Adam | epoch: 008 | loss: 52.00125 - acc: 0.0007 -- iter: 2560/2623\n",
            "Training Step: 48  | total loss: \u001b[1m\u001b[32m51.79266\u001b[0m\u001b[0m | time: 1.491s\n",
            "| Adam | epoch: 008 | loss: 51.79266 - acc: 0.0015 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 49  | total loss: \u001b[1m\u001b[32m51.43388\u001b[0m\u001b[0m | time: 0.187s\n",
            "| Adam | epoch: 009 | loss: 51.43388 - acc: 0.0013 -- iter: 0512/2623\n",
            "Training Step: 50  | total loss: \u001b[1m\u001b[32m51.12057\u001b[0m\u001b[0m | time: 0.432s\n",
            "| Adam | epoch: 009 | loss: 51.12057 - acc: 0.0011 -- iter: 1024/2623\n",
            "Training Step: 51  | total loss: \u001b[1m\u001b[32m50.98703\u001b[0m\u001b[0m | time: 0.701s\n",
            "| Adam | epoch: 009 | loss: 50.98703 - acc: 0.0009 -- iter: 1536/2623\n",
            "Training Step: 52  | total loss: \u001b[1m\u001b[32m50.82843\u001b[0m\u001b[0m | time: 0.955s\n",
            "| Adam | epoch: 009 | loss: 50.82843 - acc: 0.0011 -- iter: 2048/2623\n",
            "Training Step: 53  | total loss: \u001b[1m\u001b[32m50.66993\u001b[0m\u001b[0m | time: 1.225s\n",
            "| Adam | epoch: 009 | loss: 50.66993 - acc: 0.0009 -- iter: 2560/2623\n",
            "Training Step: 54  | total loss: \u001b[1m\u001b[32m50.47938\u001b[0m\u001b[0m | time: 1.499s\n",
            "| Adam | epoch: 009 | loss: 50.47938 - acc: 0.0008 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 55  | total loss: \u001b[1m\u001b[32m50.55389\u001b[0m\u001b[0m | time: 0.193s\n",
            "| Adam | epoch: 010 | loss: 50.55389 - acc: 0.0010 -- iter: 0512/2623\n",
            "Training Step: 56  | total loss: \u001b[1m\u001b[32m50.33104\u001b[0m\u001b[0m | time: 0.392s\n",
            "| Adam | epoch: 010 | loss: 50.33104 - acc: 0.0008 -- iter: 1024/2623\n",
            "Training Step: 57  | total loss: \u001b[1m\u001b[32m50.13757\u001b[0m\u001b[0m | time: 0.641s\n",
            "| Adam | epoch: 010 | loss: 50.13757 - acc: 0.0007 -- iter: 1536/2623\n",
            "Training Step: 58  | total loss: \u001b[1m\u001b[32m50.18973\u001b[0m\u001b[0m | time: 0.875s\n",
            "| Adam | epoch: 010 | loss: 50.18973 - acc: 0.0011 -- iter: 2048/2623\n",
            "Training Step: 59  | total loss: \u001b[1m\u001b[32m50.04333\u001b[0m\u001b[0m | time: 1.150s\n",
            "| Adam | epoch: 010 | loss: 50.04333 - acc: 0.0015 -- iter: 2560/2623\n",
            "Training Step: 60  | total loss: \u001b[1m\u001b[32m50.05714\u001b[0m\u001b[0m | time: 1.428s\n",
            "| Adam | epoch: 010 | loss: 50.05714 - acc: 0.0016 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 61  | total loss: \u001b[1m\u001b[32m50.15077\u001b[0m\u001b[0m | time: 0.267s\n",
            "| Adam | epoch: 011 | loss: 50.15077 - acc: 0.0014 -- iter: 0512/2623\n",
            "Training Step: 62  | total loss: \u001b[1m\u001b[32m50.15381\u001b[0m\u001b[0m | time: 0.464s\n",
            "| Adam | epoch: 011 | loss: 50.15381 - acc: 0.0012 -- iter: 1024/2623\n",
            "Training Step: 63  | total loss: \u001b[1m\u001b[32m50.44775\u001b[0m\u001b[0m | time: 0.647s\n",
            "| Adam | epoch: 011 | loss: 50.44775 - acc: 0.0010 -- iter: 1536/2623\n",
            "Training Step: 64  | total loss: \u001b[1m\u001b[32m50.69516\u001b[0m\u001b[0m | time: 0.883s\n",
            "| Adam | epoch: 011 | loss: 50.69516 - acc: 0.0009 -- iter: 2048/2623\n",
            "Training Step: 65  | total loss: \u001b[1m\u001b[32m50.53899\u001b[0m\u001b[0m | time: 1.129s\n",
            "| Adam | epoch: 011 | loss: 50.53899 - acc: 0.0010 -- iter: 2560/2623\n",
            "Training Step: 66  | total loss: \u001b[1m\u001b[32m50.42221\u001b[0m\u001b[0m | time: 1.389s\n",
            "| Adam | epoch: 011 | loss: 50.42221 - acc: 0.0014 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 67  | total loss: \u001b[1m\u001b[32m50.31522\u001b[0m\u001b[0m | time: 0.255s\n",
            "| Adam | epoch: 012 | loss: 50.31522 - acc: 0.0183 -- iter: 0512/2623\n",
            "Training Step: 68  | total loss: \u001b[1m\u001b[32m50.35755\u001b[0m\u001b[0m | time: 0.545s\n",
            "| Adam | epoch: 012 | loss: 50.35755 - acc: 0.0305 -- iter: 1024/2623\n",
            "Training Step: 69  | total loss: \u001b[1m\u001b[32m50.34058\u001b[0m\u001b[0m | time: 0.737s\n",
            "| Adam | epoch: 012 | loss: 50.34058 - acc: 0.0438 -- iter: 1536/2623\n",
            "Training Step: 70  | total loss: \u001b[1m\u001b[32m50.36007\u001b[0m\u001b[0m | time: 0.926s\n",
            "| Adam | epoch: 012 | loss: 50.36007 - acc: 0.0516 -- iter: 2048/2623\n",
            "Training Step: 71  | total loss: \u001b[1m\u001b[32m50.37484\u001b[0m\u001b[0m | time: 1.155s\n",
            "| Adam | epoch: 012 | loss: 50.37484 - acc: 0.0584 -- iter: 2560/2623\n",
            "Training Step: 72  | total loss: \u001b[1m\u001b[32m50.32264\u001b[0m\u001b[0m | time: 1.412s\n",
            "| Adam | epoch: 012 | loss: 50.32264 - acc: 0.0711 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 73  | total loss: \u001b[1m\u001b[32m50.34894\u001b[0m\u001b[0m | time: 0.260s\n",
            "| Adam | epoch: 013 | loss: 50.34894 - acc: 0.0802 -- iter: 0512/2623\n",
            "Training Step: 74  | total loss: \u001b[1m\u001b[32m50.20152\u001b[0m\u001b[0m | time: 0.522s\n",
            "| Adam | epoch: 013 | loss: 50.20152 - acc: 0.0870 -- iter: 1024/2623\n",
            "Training Step: 75  | total loss: \u001b[1m\u001b[32m50.12180\u001b[0m\u001b[0m | time: 0.768s\n",
            "| Adam | epoch: 013 | loss: 50.12180 - acc: 0.0930 -- iter: 1536/2623\n",
            "Training Step: 76  | total loss: \u001b[1m\u001b[32m50.21428\u001b[0m\u001b[0m | time: 0.960s\n",
            "| Adam | epoch: 013 | loss: 50.21428 - acc: 0.0986 -- iter: 2048/2623\n",
            "Training Step: 77  | total loss: \u001b[1m\u001b[32m49.92397\u001b[0m\u001b[0m | time: 1.143s\n",
            "| Adam | epoch: 013 | loss: 49.92397 - acc: 0.0881 -- iter: 2560/2623\n",
            "Training Step: 78  | total loss: \u001b[1m\u001b[32m49.66544\u001b[0m\u001b[0m | time: 1.378s\n",
            "| Adam | epoch: 013 | loss: 49.66544 - acc: 0.0789 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 79  | total loss: \u001b[1m\u001b[32m49.71062\u001b[0m\u001b[0m | time: 0.237s\n",
            "| Adam | epoch: 014 | loss: 49.71062 - acc: 0.0707 -- iter: 0512/2623\n",
            "Training Step: 80  | total loss: \u001b[1m\u001b[32m49.81888\u001b[0m\u001b[0m | time: 0.494s\n",
            "| Adam | epoch: 014 | loss: 49.81888 - acc: 0.0635 -- iter: 1024/2623\n",
            "Training Step: 81  | total loss: \u001b[1m\u001b[32m49.85564\u001b[0m\u001b[0m | time: 0.757s\n",
            "| Adam | epoch: 014 | loss: 49.85564 - acc: 0.0571 -- iter: 1536/2623\n",
            "Training Step: 82  | total loss: \u001b[1m\u001b[32m49.89694\u001b[0m\u001b[0m | time: 1.006s\n",
            "| Adam | epoch: 014 | loss: 49.89694 - acc: 0.0518 -- iter: 2048/2623\n",
            "Training Step: 83  | total loss: \u001b[1m\u001b[32m49.83985\u001b[0m\u001b[0m | time: 1.193s\n",
            "| Adam | epoch: 014 | loss: 49.83985 - acc: 0.0470 -- iter: 2560/2623\n",
            "Training Step: 84  | total loss: \u001b[1m\u001b[32m49.97688\u001b[0m\u001b[0m | time: 1.372s\n",
            "| Adam | epoch: 014 | loss: 49.97688 - acc: 0.0423 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 85  | total loss: \u001b[1m\u001b[32m50.09737\u001b[0m\u001b[0m | time: 0.250s\n",
            "| Adam | epoch: 015 | loss: 50.09737 - acc: 0.0381 -- iter: 0512/2623\n",
            "Training Step: 86  | total loss: \u001b[1m\u001b[32m49.99310\u001b[0m\u001b[0m | time: 0.492s\n",
            "| Adam | epoch: 015 | loss: 49.99310 - acc: 0.0342 -- iter: 1024/2623\n",
            "Training Step: 87  | total loss: \u001b[1m\u001b[32m49.94893\u001b[0m\u001b[0m | time: 0.751s\n",
            "| Adam | epoch: 015 | loss: 49.94893 - acc: 0.0310 -- iter: 1536/2623\n",
            "Training Step: 88  | total loss: \u001b[1m\u001b[32m49.99482\u001b[0m\u001b[0m | time: 1.020s\n",
            "| Adam | epoch: 015 | loss: 49.99482 - acc: 0.0279 -- iter: 2048/2623\n",
            "Training Step: 89  | total loss: \u001b[1m\u001b[32m49.97766\u001b[0m\u001b[0m | time: 1.293s\n",
            "| Adam | epoch: 015 | loss: 49.97766 - acc: 0.0251 -- iter: 2560/2623\n",
            "Training Step: 90  | total loss: \u001b[1m\u001b[32m50.00250\u001b[0m\u001b[0m | time: 1.482s\n",
            "| Adam | epoch: 015 | loss: 50.00250 - acc: 0.0226 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 91  | total loss: \u001b[1m\u001b[32m49.69984\u001b[0m\u001b[0m | time: 0.189s\n",
            "| Adam | epoch: 016 | loss: 49.69984 - acc: 0.0204 -- iter: 0512/2623\n",
            "Training Step: 92  | total loss: \u001b[1m\u001b[32m49.42325\u001b[0m\u001b[0m | time: 0.437s\n",
            "| Adam | epoch: 016 | loss: 49.42325 - acc: 0.0183 -- iter: 1024/2623\n",
            "Training Step: 93  | total loss: \u001b[1m\u001b[32m49.48382\u001b[0m\u001b[0m | time: 0.655s\n",
            "| Adam | epoch: 016 | loss: 49.48382 - acc: 0.0171 -- iter: 1536/2623\n",
            "Training Step: 94  | total loss: \u001b[1m\u001b[32m49.58781\u001b[0m\u001b[0m | time: 0.903s\n",
            "| Adam | epoch: 016 | loss: 49.58781 - acc: 0.0156 -- iter: 2048/2623\n",
            "Training Step: 95  | total loss: \u001b[1m\u001b[32m49.71946\u001b[0m\u001b[0m | time: 1.156s\n",
            "| Adam | epoch: 016 | loss: 49.71946 - acc: 0.0142 -- iter: 2560/2623\n",
            "Training Step: 96  | total loss: \u001b[1m\u001b[32m49.67968\u001b[0m\u001b[0m | time: 1.431s\n",
            "| Adam | epoch: 016 | loss: 49.67968 - acc: 0.0130 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 97  | total loss: \u001b[1m\u001b[32m49.61330\u001b[0m\u001b[0m | time: 0.185s\n",
            "| Adam | epoch: 017 | loss: 49.61330 - acc: 0.0119 -- iter: 0512/2623\n",
            "Training Step: 98  | total loss: \u001b[1m\u001b[32m49.76404\u001b[0m\u001b[0m | time: 0.375s\n",
            "| Adam | epoch: 017 | loss: 49.76404 - acc: 0.0107 -- iter: 1024/2623\n",
            "Training Step: 99  | total loss: \u001b[1m\u001b[32m49.89937\u001b[0m\u001b[0m | time: 0.654s\n",
            "| Adam | epoch: 017 | loss: 49.89937 - acc: 0.0096 -- iter: 1536/2623\n",
            "Training Step: 100  | total loss: \u001b[1m\u001b[32m49.88154\u001b[0m\u001b[0m | time: 0.917s\n",
            "| Adam | epoch: 017 | loss: 49.88154 - acc: 0.0087 -- iter: 2048/2623\n",
            "Training Step: 101  | total loss: \u001b[1m\u001b[32m49.86939\u001b[0m\u001b[0m | time: 1.209s\n",
            "| Adam | epoch: 017 | loss: 49.86939 - acc: 0.0080 -- iter: 2560/2623\n",
            "Training Step: 102  | total loss: \u001b[1m\u001b[32m49.95301\u001b[0m\u001b[0m | time: 1.471s\n",
            "| Adam | epoch: 017 | loss: 49.95301 - acc: 0.0076 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 103  | total loss: \u001b[1m\u001b[32m49.96255\u001b[0m\u001b[0m | time: 0.268s\n",
            "| Adam | epoch: 018 | loss: 49.96255 - acc: 0.0068 -- iter: 0512/2623\n",
            "Training Step: 104  | total loss: \u001b[1m\u001b[32m49.96783\u001b[0m\u001b[0m | time: 0.450s\n",
            "| Adam | epoch: 018 | loss: 49.96783 - acc: 0.0063 -- iter: 1024/2623\n",
            "Training Step: 105  | total loss: \u001b[1m\u001b[32m49.74851\u001b[0m\u001b[0m | time: 0.642s\n",
            "| Adam | epoch: 018 | loss: 49.74851 - acc: 0.0057 -- iter: 1536/2623\n",
            "Training Step: 106  | total loss: \u001b[1m\u001b[32m49.55020\u001b[0m\u001b[0m | time: 0.877s\n",
            "| Adam | epoch: 018 | loss: 49.55020 - acc: 0.0051 -- iter: 2048/2623\n",
            "Training Step: 107  | total loss: \u001b[1m\u001b[32m49.53218\u001b[0m\u001b[0m | time: 1.129s\n",
            "| Adam | epoch: 018 | loss: 49.53218 - acc: 0.0046 -- iter: 2560/2623\n",
            "Training Step: 108  | total loss: \u001b[1m\u001b[32m49.54005\u001b[0m\u001b[0m | time: 1.381s\n",
            "| Adam | epoch: 018 | loss: 49.54005 - acc: 0.0043 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 109  | total loss: \u001b[1m\u001b[32m49.60285\u001b[0m\u001b[0m | time: 0.277s\n",
            "| Adam | epoch: 019 | loss: 49.60285 - acc: 0.0039 -- iter: 0512/2623\n",
            "Training Step: 110  | total loss: \u001b[1m\u001b[32m49.66549\u001b[0m\u001b[0m | time: 0.538s\n",
            "| Adam | epoch: 019 | loss: 49.66549 - acc: 0.0037 -- iter: 1024/2623\n",
            "Training Step: 111  | total loss: \u001b[1m\u001b[32m49.60759\u001b[0m\u001b[0m | time: 0.719s\n",
            "| Adam | epoch: 019 | loss: 49.60759 - acc: 0.0037 -- iter: 1536/2623\n",
            "Training Step: 112  | total loss: \u001b[1m\u001b[32m49.92293\u001b[0m\u001b[0m | time: 0.906s\n",
            "| Adam | epoch: 019 | loss: 49.92293 - acc: 0.0034 -- iter: 2048/2623\n",
            "Training Step: 113  | total loss: \u001b[1m\u001b[32m50.20358\u001b[0m\u001b[0m | time: 1.151s\n",
            "| Adam | epoch: 019 | loss: 50.20358 - acc: 0.0030 -- iter: 2560/2623\n",
            "Training Step: 114  | total loss: \u001b[1m\u001b[32m50.20383\u001b[0m\u001b[0m | time: 1.414s\n",
            "| Adam | epoch: 019 | loss: 50.20383 - acc: 0.0027 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 115  | total loss: \u001b[1m\u001b[32m50.20234\u001b[0m\u001b[0m | time: 0.259s\n",
            "| Adam | epoch: 020 | loss: 50.20234 - acc: 0.0025 -- iter: 0512/2623\n",
            "Training Step: 116  | total loss: \u001b[1m\u001b[32m50.14608\u001b[0m\u001b[0m | time: 0.529s\n",
            "| Adam | epoch: 020 | loss: 50.14608 - acc: 0.0024 -- iter: 1024/2623\n",
            "Training Step: 117  | total loss: \u001b[1m\u001b[32m50.16666\u001b[0m\u001b[0m | time: 0.794s\n",
            "| Adam | epoch: 020 | loss: 50.16666 - acc: 0.0022 -- iter: 1536/2623\n",
            "Training Step: 118  | total loss: \u001b[1m\u001b[32m50.13989\u001b[0m\u001b[0m | time: 0.974s\n",
            "| Adam | epoch: 020 | loss: 50.13989 - acc: 0.0021 -- iter: 2048/2623\n",
            "Training Step: 119  | total loss: \u001b[1m\u001b[32m50.08557\u001b[0m\u001b[0m | time: 1.167s\n",
            "| Adam | epoch: 020 | loss: 50.08557 - acc: 0.0019 -- iter: 2560/2623\n",
            "Training Step: 120  | total loss: \u001b[1m\u001b[32m50.03608\u001b[0m\u001b[0m | time: 1.413s\n",
            "| Adam | epoch: 020 | loss: 50.03608 - acc: 0.0017 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 121  | total loss: \u001b[1m\u001b[32m49.99529\u001b[0m\u001b[0m | time: 0.251s\n",
            "| Adam | epoch: 021 | loss: 49.99529 - acc: 0.0020 -- iter: 0512/2623\n",
            "Training Step: 122  | total loss: \u001b[1m\u001b[32m49.92022\u001b[0m\u001b[0m | time: 0.510s\n",
            "| Adam | epoch: 021 | loss: 49.92022 - acc: 0.0018 -- iter: 1024/2623\n",
            "Training Step: 123  | total loss: \u001b[1m\u001b[32m49.90042\u001b[0m\u001b[0m | time: 0.781s\n",
            "| Adam | epoch: 021 | loss: 49.90042 - acc: 0.0018 -- iter: 1536/2623\n",
            "Training Step: 124  | total loss: \u001b[1m\u001b[32m49.88195\u001b[0m\u001b[0m | time: 1.033s\n",
            "| Adam | epoch: 021 | loss: 49.88195 - acc: 0.0016 -- iter: 2048/2623\n",
            "Training Step: 125  | total loss: \u001b[1m\u001b[32m50.00269\u001b[0m\u001b[0m | time: 1.224s\n",
            "| Adam | epoch: 021 | loss: 50.00269 - acc: 0.0016 -- iter: 2560/2623\n",
            "Training Step: 126  | total loss: \u001b[1m\u001b[32m50.01407\u001b[0m\u001b[0m | time: 1.409s\n",
            "| Adam | epoch: 021 | loss: 50.01407 - acc: 0.0015 -- iter: 2623/2623\n",
            "--\n",
            "Training Step: 127  | total loss: \u001b[1m\u001b[32m50.01767\u001b[0m\u001b[0m | time: 0.239s\n",
            "| Adam | epoch: 022 | loss: 50.01767 - acc: 0.0013 -- iter: 0512/2623\n",
            "Training Step: 128  | total loss: \u001b[1m\u001b[32m49.99657\u001b[0m\u001b[0m | time: 0.475s\n",
            "| Adam | epoch: 022 | loss: 49.99657 - acc: 0.0016 -- iter: 1024/2623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGl9wYhICpP5"
      },
      "source": [
        "# Here you can change the paths to other trained models / other song files\n",
        "load_generative_seeds_from = song_name\n",
        "load_model_from = model_name+\".tfl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rIKifb7Cp1F"
      },
      "source": [
        "from server_handler import ServerHandler\n",
        "import settings\n",
        "\n",
        "my_settings = settings.Settings(args)\n",
        "print(\"Important Settings: settings.lstm_layers=\", my_settings.lstm_layers, \", settings.lstm_units=\", my_settings.lstm_units,\n",
        "              \", settings.sample_rate=\", my_settings.sample_rate)\n",
        "\n",
        "generation_handler = ServerHandler(my_settings, manual_loading = True)\n",
        "generation_handler.manual_init_song_model(load_generative_seeds_from, load_model_from)\n",
        "\n",
        "# slightly experimental interpolation through the latents while generating ...\n",
        "\n",
        "generation_handler.change_impulse(0.2) # set to 20% sharp\n",
        "\n",
        "sequence = [\n",
        "  #Starts generating 10% through the song and generates 200 frames \n",
        "  [0.1, 200],\n",
        "  #Moves to 60% through the song and generates 300 frames\n",
        "  [0.6, 300],\n",
        "  #Moves to 90% through the song and generates 150 frames\n",
        "  [0.9, 150]            \n",
        "]\n",
        "\n",
        "output_audio = []\n",
        "\n",
        "for i in sequence:\n",
        "  position_in_the_song = i[0]\n",
        "  requested_length = i[1]\n",
        "  generation_handler.change_impulse_smoothly_start(position_in_the_song) # allow interpolation\n",
        "  audio, predict, reconstruct = generation_handler.generate_audio_sample(requested_length, interactive_i=position_in_the_song)\n",
        "  output_audio.append(audio)\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvkLbeHtCtKO"
      },
      "source": [
        "import librosa\n",
        "from IPython.display import Audio, Image\n",
        "output_audio = np.concatenate(output_audio)\n",
        "\n",
        "out_name = 'generated_output_exp_concat.wav'\n",
        "librosa.output.write_wav(out_name, output_audio, sr=sample_rate)\n",
        "Audio(out_name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}